{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JK3oHw21_3ua",
        "outputId": "faa2379b-0aba-4103-af98-f07d4053fb6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of instances: 50\n",
            "| Classifier                | Training Accuracy | Test Accuracy |\n",
            "| ------------------------- | ----------------- | ------------- |\n",
            "| SVC                       |         0.7750000 |     0.0000000 |\n",
            "| LinearSVC                 |         0.9750000 |     0.3000000 |\n",
            "| RandomForestClassifier    |         0.9750000 |     0.1000000 |\n",
            "| DecisionTreeClassifier    |         0.9750000 |     0.2000000 |\n",
            "During the period of falling in love, each time that we met and especially when we had not met for a long time. ðŸ˜‚\n",
            "When I was involved in a traffic accident. ðŸ˜±\n",
            "When I was driving home after several days of hard work, there was a motorist ahead of me who was driving at 50 km/hour and refused, despite his low speed, to let me overtake. ðŸ˜¡\n",
            "When I lost the person who meant the most to me. ðŸ˜¥\n",
            "The time I knocked a deer down - the sight of the animal's injuries and helplessness. The realization that the animal was so badly hurt that it had to be put down, and when the animal screamed at the moment of death. ðŸ¤¢\n",
            "When I did not speak the truth. ðŸ˜³\n",
            "When I caused problems for somebody because he could not keep the appointed time and this led to various consequences. ðŸ˜“\n",
            "Opening the gift, I found a plane ticket to my dream destination. ðŸ˜®\n",
            "The countdown has begun, and I can't wait for the much-anticipated reunion with my best friends. ðŸ˜±\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')#divides a text into a list of sentences\n",
        "nltk.download('wordnet')\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def read_data(file):\n",
        "    data = []\n",
        "    with open('/content/text 2.txt',) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            label = ' '.join(line[1:line.find(\"]\")].strip().split())\n",
        "            text = line[line.find(\"]\") + 1:].strip()\n",
        "            data.append([label, text])\n",
        "    return data\n",
        "\n",
        "file = 'text 2.txt'\n",
        "data = read_data(file)\n",
        "print(\"Number of instances: {}\".format(len(data)))\n",
        "def ngram(token, n):\n",
        "   output = []\n",
        "   for i in range(n-1, len(token)):\n",
        "       ngram = ' '.join(token[i-n+1:i+1])\n",
        "       output.append(ngram)\n",
        "   return output\n",
        "def create_feature(text, nrange=(1, 1)):\n",
        "   text_features = []\n",
        "   text = text.lower()\n",
        "   text_alphanum = re.sub('[^a-z0-9#]', ' ', text)\n",
        "   for n in range(nrange[0], nrange[1]+1):\n",
        "     text_features += ngram(text_alphanum.split(), n)\n",
        "   text_punc = re.sub('[a-z0-9]', ' ', text)\n",
        "   text_features += ngram(text_punc.split(), 1)\n",
        "   return Counter(text_features)\n",
        "def convert_label(item, name):\n",
        "  items = list(map(float, item.split()))\n",
        "  label = \"\"\n",
        "  for idx in range(len(items)):\n",
        "    if items[idx] == 1:\n",
        "       label += name[idx] + \" \"\n",
        "  return label.strip()\n",
        "emotions = [\"joy\", 'fear', \"anger\", \"sadness\", \"disgust\",\n",
        "\"shame\", \"guilt\" ,\"surprise\" , \"anticipation\"]\n",
        "X_all = []\n",
        "y_all = []\n",
        "for label, text in data:\n",
        "   y_all.append(convert_label(label, emotions))\n",
        "   X_all.append(create_feature(text, nrange=(1, 4)))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all,\n",
        "test_size = 0.2, random_state = 123)\n",
        "def train_test(clf, X_train, X_test, y_train, y_test):\n",
        "  clf.fit(X_train, y_train)\n",
        "  train_acc = accuracy_score(y_train, clf.predict(X_train))\n",
        "  test_acc = accuracy_score(y_test, clf.predict(X_test))\n",
        "  return train_acc, test_acc\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "vectorizer = DictVectorizer(sparse = True)\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "svc = SVC()\n",
        "lsvc = LinearSVC(random_state=123)\n",
        "rforest = RandomForestClassifier(random_state=123)\n",
        "dtree = DecisionTreeClassifier()\n",
        "clifs = [svc, lsvc, rforest, dtree]\n",
        "\n",
        "# train and test them\n",
        "print(\"| {:25} | {} | {} |\".format(\"Classifier\", \"Training Accuracy\", \"Test Accuracy\"))\n",
        "print(\"| {} | {} | {} |\".format(\"-\"*25, \"-\"*17, \"-\"*13))\n",
        "for clf in clifs:\n",
        "    clf_name = clf.__class__.__name__\n",
        "    train_acc, test_acc = train_test(clf, X_train, X_test, y_train, y_test)\n",
        "    print(\"| {:25} | {:17.7f} | {:13.7f} |\".format(clf_name, train_acc, test_acc))\n",
        "\n",
        "emoji_dict = {\"joy\": \"ðŸ˜‚\", \"fear\": \"ðŸ˜±\", \"anger\": \"ðŸ˜¡\",\n",
        "              \"sadness\": \"ðŸ˜¥\", \"disgust\": \"ðŸ¤¢\", \"shame\": \"ðŸ˜³\",\n",
        "              \"guilt\": \"ðŸ˜“\", \"surprise\": \"ðŸ˜®\", \"anticipation\": \"ðŸ˜ƒ\"}\n",
        "\n",
        "t1 =\"During the period of falling in love, each time that we met and especially when we had not met for a long time.\"\n",
        "t2 =\"When I was involved in a traffic accident.\"\n",
        "t3 =\"When I was driving home after several days of hard work, there was a motorist ahead of me who was driving at 50 km/hour and refused, despite his low speed, to let me overtake.\"\n",
        "t4 =\"When I lost the person who meant the most to me.\"\n",
        "t5 =\"The time I knocked a deer down - the sight of the animal's injuries and helplessness. The realization that the animal was so badly hurt that it had to be put down, and when the animal screamed at the moment of death.\"\n",
        "t6 = \"When I did not speak the truth.\"\n",
        "t7 =\"When I caused problems for somebody because he could not keep the appointed time and this led to various consequences.\"\n",
        "t8 =\"Opening the gift, I found a plane ticket to my dream destination.\"\n",
        "t9 =\"The countdown has begun, and I can't wait for the much-anticipated reunion with my best friends.\"\n",
        "texts = [t1, t2, t3, t4 ,t5 ,t6 ,t7 ,t8 ,t9]\n",
        "\n",
        "\n",
        "for text in texts:\n",
        "  features = create_feature(text, nrange=(1, 4))\n",
        "  features = vectorizer.transform(features)\n",
        "  prediction = clf.predict(features)[0]\n",
        "  print( text,emoji_dict[prediction])"
      ]
    }
  ]
}